# Mini Data Pipeline

Проект демонстрирует работу простого конвейера обработки данных с использованием Apache Kafka, PostgreSQL и FastAPI. В проекте реализованы следующие компоненты:

- **Producer**: Чтение логов из файла и отправка сообщений в Kafka.
- **Consumer**: Чтение сообщений из Kafka и запись их в базу данных PostgreSQL.
- **API**: REST API для получения логов и аналитики.

## Содержание

- [Требования](#требования)
- [Установка и настройка](#установка-и-настройка)
  - [1. Клонирование репозитория](#1-клонирование-репозитория)
  - [2. Создание виртуального окружения и установка зависимостей](#2-создание-виртуального-окружения-и-установка-зависимостей)
  - [3. Настройка переменных окружения](#3-настройка-переменных-окружения)
  - [4. Поднятие инфраструктуры с помощью Docker Compose](#4-поднятие-инфраструктуры-с-помощью-docker-compose)
- [Запуск компонентов](#запуск-компонентов)
  - [Запуск Consumer](#запуск-consumer)
  - [Запуск Producer](#запуск-producer)
  - [Запуск API](#запуск-api)
- [Использование API](#использование-api)
- [Структура проекта](#структура-проекта)
- [Дополнительная информация](#дополнительная-информация)

## Требования

- **Python 3.6+**
- **Docker** и **Docker Compose**
- **Git**

## Установка и настройка

### 1. Клонирование репозитория

Клонируйте репозиторий на ваш локальный компьютер:


git clone https://github.com/ВашеИмяПользователя/mini-data-pipeline.git
cd mini-data-pipeline

###  2. Создание виртуального окружения и установка зависимостей
Создайте виртуальное окружение и установите все необходимые зависимости.

На Windows:
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
На macOS/Linux:

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

###  3. Настройка переменных окружения
В корне проекта находится файл .env, содержащий настройки для подключения к Kafka и PostgreSQL. При необходимости отредактируйте его:

###  4. Поднятие инфраструктуры с помощью Docker Compose
Для работы проекта требуются контейнеры с Kafka, Zookeeper и PostgreSQL. Убедитесь, что Docker установлен и запущен, затем выполните:


docker compose up -d
Примечание: Если ваша система использует старую версию Docker Compose, команда может выглядеть так:


docker-compose up -d
Запуск компонентов
### Запуск Consumer
Consumer читает сообщения из Kafka и записывает их в базу данных PostgreSQL. Рекомендуется запускать его как модуль из корневой директории проекта:


python -m consumer.consumer
### Запуск Producer
Producer читает лог-файл logs.txt и отправляет сообщения в Kafka. Запустите его следующим образом:


python -m producer.producer
### Запуск API
API-сервер на FastAPI предоставляет доступ к логам и аналитике. Для его запуска выполните команду:


uvicorn api.main:app --reload
После запуска API откройте браузер и перейдите по адресу http://127.0.0.1:8000/docs для просмотра интерактивной документации.

### Использование API
В API доступны следующие эндпоинты:

GET /logs: Получить список всех логов, сохранённых в базе данных.
GET /stats: Получить сводную статистику (например, общее количество логов).
Примеры запросов:

Получение логов:


curl http://127.0.0.1:8000/logs
Получение статистики:


curl http://127.0.0.1:8000/stats

### Дополнительная информация
Docker и Docker Compose: Убедитесь, что Docker Desktop запущен перед поднятием контейнеров.
Переменные окружения: При необходимости измените параметры подключения в файле .env.
Запуск скриптов: Рекомендуется запускать скрипты как модули (с использованием python -m ...), чтобы избежать проблем с импортами.
Логи и отладка: Логи работы компонентов выводятся в консоль. При необходимости можно добавить дополнительное логирование для отладки.
